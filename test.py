#!/usr/bin/env python3
import newspaper
import os

from time import sleep
from datetime import datetime, timedelta
from newspaper import Article
from bs4 import BeautifulSoup
from collections import Counter
from itertools import count
from random import choice
from requests import get, codes
from selenium import webdriver
from seleniumrequests import Chrome

USER_AGENTS = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20100101 Firefox/11.0',
               'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:22.0) Gecko/20100 101 Firefox/22.0',
               'Mozilla/5.0 (Windows NT 6.1; rv:11.0) Gecko/20100101 Firefox/11.0',
               ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.5 (KHTML, like Gecko  ) '
                'Chrome/19.0.1084.46 Safari/536.5'),
               ('Mozilla/5.0 (Windows; Windows NT 6.1) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/  19.0.1084.46'
                'Safari/536.5'), )
now = datetime.now()
chromedriver_path = os.environ.get('CHROMEDRIVER_PATH')
driver = webdriver.Chrome(chromedriver_path)
# now = datetime.now() - timedelta(days=1)


def get_news_article_info(url):
    article = Article(url)
    article.download()
    try:
        article.parse()
    except newspaper.article.ArticleException:
        return None
    except UnicodeEncodeError:
        return None
    article.nlp()
    return article.keywords, article.title, article.summary


def check_valid_string(text):
    text = text.strip()
    text = text.replace("'", "").replace('"', '').replace('Â·', ',')
    return text


def match_soup_class(target, mode='class'):
    def do_match(tag):
        classes = tag.get(mode, [])
        return all(c in classes for c in target)
    return do_match


def request_and_get(url):
    try:
        r = get(url, headers={'User-Agent': choice(USER_AGENTS)})
        if r.status_code != codes.ok:
            print('[%s] request error, code=%d', url, r.status_code)
            return False
        return r
    except TypeError:
        print('[%s] connect fail', url)
        return False


def realestate_gyunghyang(keywords_list):
    cnt = 0
    r = request_and_get('http://biz.khan.co.kr/khan_art_list.html?category=realty')
    if r is None:
        return
    today = '%4d. %02d. %02d' % (now.year, now.month, now.day)

    soup = BeautifulSoup(r.content.decode('euc-kr', 'replace'), 'html.parser')
    for news_list in soup.find_all(match_soup_class(['news_list'])):
        for li in news_list.find_all('li'):
            try:
                article_date = li.find('em', attrs={'class': 'letter'}).text
                if not article_date.startswith(today):
                    continue
                if cnt == 0:
                    print('\nğŸ“° ê²½í–¥ì‹ ë¬¸')
                cnt += 1
                title = li.find('strong', attrs={'class': 'hd_title'})
                print(title.text)
                print(title.a['href'])
                keywords = get_news_article_info(title.a['href'])
                keywords_list.extend(keywords)
            except TypeError:
                continue


def realestate_kookmin(keywords_list):
    cnt = 0
    r = request_and_get('http://news.kmib.co.kr/article/list.asp?sid1=eco')
    if r is None:
        return
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)

    base_url = 'http://news.kmib.co.kr/article'
    soup = BeautifulSoup(r.content.decode('euc-kr', 'replace'), 'html.parser')
    cnt = 0
    for nws_list in soup.find_all(match_soup_class(['nws_list'])):
        for dl in nws_list.find_all('dl'):
            article_date = dl.find('dd', attrs={'class': 'date'}).text
            if not article_date.startswith(today):
                continue
            if dl.text == 'ë“±ë¡ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.':
                return
            dt = dl.find('dt')
            href = '%s/%s' % (base_url, dt.a['href'])
            title = check_valid_string(dt.a.text)
            if (title.find('ì•„íŒŒíŠ¸') != -1 or
               title.find('êµ­í† ë¶€') != -1 or title.find('êµ­í† êµí†µë¶€') != -1 or
               title.find('ì „ì„¸') != -1 or title.find('ì „ì›”ì„¸') != -1 or
               title.find('ì²­ì•½') != -1 or title.find('ë¶„ì–‘') != -1 or
               title.find('ë¶€ë™ì‚°') != -1):

                if cnt == 0:
                    print('\nğŸ“° êµ­ë¯¼ì¼ë³´')
                print(title)
                print(href)
                keywords = get_news_article_info(href)
                keywords_list.extend(keywords)
                cnt += 1


def realestate_nocut(keywords_list):
    cnt = 0
    r = request_and_get('http://www.nocutnews.co.kr/news/list?c1=203&c2=204&ltype=1')
    if r is None:
        return
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)

    base_url = 'http://www.nocutnews.co.kr'
    soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
    news = soup.find(match_soup_class(['newslist']))
    for dl in news.find_all('dl'):
        dt = dl.find('dt')
        href = '%s%s' % (base_url, dt.a['href'])
        title = check_valid_string(dt.text)
        temp = (dl.find('dd', attrs={'class': 'txt'}).text).split(' ')
        article_date = ''.join(temp[-3:])
        if not article_date.startswith(today):
            continue
        if cnt == 0:
            print('\nğŸ“° ë…¸ì»·ë‰´ìŠ¤')
        print(title)
        print(href)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)
    return


def realestate_donga(keywords_list):
    cnt = 0
    r = request_and_get('http://news.donga.com/List/Economy/RE')
    if r is None:
        return
    today = '%4d%02d%02d' % (now.year, now.month, now.day)

    soup = BeautifulSoup(r.text, 'html.parser')
    for alist in soup.find_all(match_soup_class(['articleList'])):
        tit = alist.find('span', attrs={'class': 'tit'})
        title = check_valid_string(tit.text)
        temp = (alist.a['href']).split('/')
        article_date = temp[-3]
        if not article_date.startswith(today):
            continue
        if cnt == 0:
            print('\nğŸ“° ë™ì•„ì¼ë³´')
        print(title)
        print(alist.a['href'])
        keywords = get_news_article_info(alist.a['href'])
        keywords_list.extend(keywords)
        cnt += 1


def realestate_mbn(keywords_list):
    cnt = 0
    r = request_and_get('http://news.mk.co.kr/newsList.php?sc=30000020')
    if r is None:
        return
    today = '%4d.%02d.%02d' % (now.year, now.month, now.day)

    soup = BeautifulSoup(r.content.decode('euc-kr', 'replace'), 'html.parser')
    for list_area in soup.find_all(match_soup_class(['list_area'])):
        for dl in list_area.find_all('dl'):
            dt = dl.find('dt')
            href = dt.a['href']
            title = check_valid_string(dt.text)
            article_date = dl.find('span', attrs={'class': 'date'}).text
            if not article_date.startswith(today):
                continue
            if cnt == 0:
                print('\nğŸ“° ë§¤ì¼ê²½ì œ')
            print(title)
            print(href)
            cnt += 1
            keywords = get_news_article_info(href)
            keywords_list.extend(keywords)


def realestate_moonhwa(keywords_list):
    cnt = 0
    r = request_and_get('http://www.munhwa.com/news/section_list.html?sec=economy&class=5')
    if r is None:
        return
    today = '%4d.%02d.%02d' % (now.year, now.month, now.day)
    soup = BeautifulSoup(r.content.decode('euc-kr', 'replace'), 'html.parser')
    for td in soup.find_all('td', attrs={'style': 'padding:4 0 0 3'}):
        articles = td.text.split()
        article_date = articles[-1].replace(']', '').replace('[', '')
        if not article_date.startswith(today):
            continue
        if cnt == 0:
            print('\nğŸ“° ë¬¸í™”ì¼ë³´')
        cnt += 1
        print(td.a['href'])
        print(' '.join(articles[:-1]))
        keywords = get_news_article_info(td.a['href'])
        keywords_list.extend(keywords)


def realestate_segye(keywords_list):
    cnt = 0
    r = request_and_get('http://www.segye.com/newsList/0101030700000')
    if r is None:
        return
    today = '%4d%02d%02d' % (now.year, now.month, now.day)
    base_url = 'http://www.segye.com'
    soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
    for r_txt in soup.find_all(match_soup_class(['r_txt'])):
        for dt in r_txt.find_all('dt'):
            href = '%s%s' % (base_url, dt.a['href'])
            title = dt.text
            article_date = dt.a['href'].split('/')[-1]
            if not article_date.startswith(today):
                continue
            if cnt == 0:
                print('\nğŸ“° ì„¸ê³„ì¼ë³´')
            cnt += 1
            print(title)
            print(href)
            keywords = get_news_article_info(href)
            keywords_list.extend(keywords)


def realestate_joins(keywords_list):
    cnt = 0
    r = request_and_get('http://realestate.joins.com/?cloc=joongang|section|subsection')
    if r is None:
        return
    today = '%4d.%02d.%02d' % (now.year, now.month, now.day)

    base_url = 'http://news.joins.com'
    soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
    for f in soup.find_all(match_soup_class(['bd'])):
        for li in f.find_all('li'):
            try:
                title = li.a['title']
            except KeyError:
                title = check_valid_string(' '.join(li.text.strip().split()[1:-2]))
            try:
                href = '%s%s' % (base_url, li.a['href'])
            except TypeError:
                continue
            temp = li.find('span', attrs={'class': 'date'})
            try:
                article_date = temp.text
            except AttributeError:
                continue

            if not article_date.startswith(today):
                continue
            if cnt == 0:
                print('\nğŸ“° ì¤‘ì•™ì¼ë³´')
            cnt += 1
            print(title)
            print(href)
            # ì¤‘ì•™ì¼ë³´ not working
            # keywords = get_news_article_info(href)
            # keywords_list.extend(keywords)


def realestate_chosun(keywords_list):
    cnt = 0
    r = request_and_get('http://biz.chosun.com/svc/list_in/list.html?catid=4&gnb_global')
    if r is None:
        return
    today = '%4d%02d%02d' % (now.year, now.month, now.day)

    base_url = 'http://biz.chosun.com'
    soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
    for f in soup.find_all(match_soup_class(['list_vt'])):
        for li in f.find_all('li'):
            dt = li.find('dt')
            href = '%s%s' % (base_url, li.a['href'])
            title = check_valid_string(dt.a.text)
            article_date = li.a['href'].split('/')[-1]
            if not article_date.startswith(today):
                continue
            if cnt == 0:
                print('\nğŸ“° ì¡°ì„ ì¼ë³´')
            cnt += 1
            print(title)
            print(href)
            keywords = get_news_article_info(href)
            keywords_list.extend(keywords)


def realestate_hani(keywords_list):
    cnt = 0
    r = request_and_get(' http://www.hani.co.kr/arti/economy/property/home01.html')
    if r is None:
        return
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)

    base_url = 'http://www.hani.co.kr'
    soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
    for article in soup.find_all(match_soup_class(['article-area'])):
        article_date = article.find('span', attrs={'class': 'date'}).text
        href = '%s%s' % (base_url, article.a['href'])
        article = article.text.strip().split('\n')
        title = check_valid_string(article[0])
        if not article_date.startswith(today):
            continue
        if cnt == 0:
            print('\nğŸ“° í•œê²¨ë¡€ì‹ ë¬¸')
        cnt += 1
        print(title)
        print(href)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)
    return


def realestate_hankyung(keywords_list):
    cnt = 0
    r = request_and_get('http://land.hankyung.com/')
    if r is None:
        return
    today = '%4d%02d%02d' % (now.year, now.month, now.day)
    soup = BeautifulSoup(r.content.decode('euc-kr', 'replace'), 'html.parser')
    sessions = soup.select('div > h2 > a')
    for s in sessions:
        if s['href'] == 'http://www.hankyung.com/news/kisarank/':
            continue
        href = s['href']
        title = check_valid_string(s.text)
        article_date = href.split('/')[-1]
        if not article_date.startswith(today):
            continue
        if cnt == 0:
            print('\nğŸ“° í•œêµ­ê²½ì œ')
        cnt += 1
        print(title)
        print(href)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)


def realestate_naver(keywords_list):
    r = request_and_get('http://land.naver.com/news/headline.nhn')
    if r is None:
        return
    print('\nğŸ“° NAVER')

    base_url = 'http://land.naver.com'
    soup = BeautifulSoup(r.text, 'html.parser')
    sessions = soup.select('div > div > div > div > div > dl > dt > a')
    for s in sessions:
        href = '%s%s' % (base_url, s['href'])
        title = check_valid_string(s.text)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)
        print(title)
        print(href)

    sessions = soup.select('div > ul > li > dl > dt > a')
    for s in sessions:
        if len(s.text) == 0:
            continue
        href = '%s%s' % (base_url, s['href'])
        title = check_valid_string(s.text)
        print(title)
        print(href)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)
    return


def realestate_daum(keywords_list):
    print('\nğŸ“° DAUM')
    r = request_and_get('http://realestate.daum.net/news')
    soup = BeautifulSoup(r.text, 'html.parser')
    for f in soup.find_all(match_soup_class(['link_news'])):
        try:
            href = f['href']
        except TypeError:
            continue
        title = check_valid_string(f.text)
        keywords = get_news_article_info(href)
        keywords_list.extend(keywords)
        print(title)
        print(href)



def realestate_cnews(keywords_list):
    base_url = 'http://www.cnews.co.kr/uhtml/read.jsp?idxno='
    today = '%4d%02d%02d' % (now.year, now.month, now.day)
    cnt = 0
    urls = [ 
            'http://www.cnews.co.kr/uhtml/autosec/S1N1_S2N12_1.html', # ë¶„ì–‘
            'http://www.cnews.co.kr/uhtml/autosec/S1N1_S2N13_1.html', # ë„ì‹œì •ë¹„
            'http://www.cnews.co.kr/uhtml/autosec/S1N1_S2N14_1.html', # ê°œë°œ
            'http://www.cnews.co.kr/uhtml/autosec/S1N1_S2N15_1.html', # ì¬íƒœí¬
            'http://www.cnews.co.kr/uhtml/autosec/S1N1_S2N16_1.html', # ë¶€ë™ì‚°ì‹œì¥
            ]
    for url in urls:
        r = request_and_get(url)
        soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
        for sub_list in soup.find_all(match_soup_class(['sub_main_news_list_2'])):
            for li in sub_list.find_all('li'):
                title = li.find('div', {'class': 'title'})
                article_date = li.a['href'].split("'")[1]
                if not article_date.startswith(today):
                    continue
                if cnt == 0:
                    print('\nğŸ“° ê±´ì„¤ê²½ì œ')
                cnt += 1
                href = '%s%s' % (base_url, article_date)
                print(title.text)
                print(href)
                keywords = get_news_article_info(href)
                keywords_list.extend(keywords)


def test(keywords_list):
    driver = webdriver.Chrome(chromedriver_path)
    driver.implicitly_wait(3)
    driver.get('http://www.cnews.co.kr/')

    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        sleep(1)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
    print('wow')
    sleep(10)
    # driver.find_element_by_xpath('/html/body/div[2]/div[1]/div[3]/div/div[1]/div[2]/div/ul/li[2]/ul/li[1]/div/a').click()
    driver.quit()

def realestate_sedaily(keywords_list):
    urls = ['http://www.sedaily.com/NewsList/GB01',   # ì •ì±…, ì œë„
            'http://www.sedaily.com/NewsList/GB02',   # ë¶„ì–‘, ì²­ì•½
            'http://www.sedaily.com/NewsList/GB03',   # ì•„íŒŒíŠ¸, ì£¼íƒ
            'http://www.sedaily.com/NewsList/GB04',   # ì˜¤í”¼ìŠ¤, ìƒê°€, í† ì§€
            'http://www.sedaily.com/NewsList/GB05',   # ê±´ì„¤ì—…ê³„
            'http://www.sedaily.com/NewsList/GB06',   # ê°„ì ‘íˆ¬ì
            'http://www.sedaily.com/NewsList/GB07',   # ê¸°íšì—°ì¬
            ]

    base_url = 'http://www.sedaily.com'
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)
    cnt = 0
    for url in urls:
        r = request_and_get(url)
        soup = BeautifulSoup(r.content.decode('utf-8', 'replace'), 'html.parser')
        for news_list in soup.find_all(match_soup_class(['news_list'])):
            for li in news_list.find_all('li'):
                dt = li.find('dt')
                href = '%s%s' % (base_url, dt.a['href'])
                dd = li.find('dd')
                article_date = dd.find('span', attrs={'class': 'letter'}).text
                if not article_date.startswith(today):
                    continue
                if cnt == 0:
                    print('\nğŸ“° ì„œìš¸ê²½ì œ')
                cnt += 1
                print(dt.text)
                print(href)
                keywords = get_news_article_info(href)
                keywords_list.extend(keywords)


import re
def realestate_cak(keywords_list):
    r = request_and_get('http://www.cak.or.kr/board/boardList.do?boardId=news_builder&menuId=437#')
    soup = BeautifulSoup(r.text, 'html.parser')
    today = '%4d.%02d.%02d' % (now.year, now.month, now.day-2)
    for tbody in soup.find_all('tbody'):
        for tr in tbody.find_all('tr'):
            for idx, td in enumerate(tr.find_all('td')):
                if idx == 1:
                    temp = str(td.a).split('"')[3]
                    # dataView(35653); -> 35653
                    data_id = re.findall('\(.*?\d\)', temp)[0][1:-1]
                    href = 'http://www.cak.or.kr/board/boardView.do?menuId=437&cms_site_id=&sel_tab=&searchCondition=all&searchKeyword=&sidohp=&subhp=&boardId=news_builder&dataId=%s&pageIndex=1' % data_id
                    title = td.text.strip()
                    continue
                if idx == 2:
                    article_date = td.text.strip()
                    if not article_date.startswith(today):
                        continue
                    print(title)
                    print(href)


def realestate_venchosun(keywords_list):
    r = request_and_get('http://www.vanchosun.com/realestate/main/frame.php?main=newinfo')
    soup = BeautifulSoup(r.text, 'html.parser')
    #cf_padding > table > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(1) > a > span
    for idx, table in enumerate(soup.find_all('table')):
        if idx != 6:
            continue
        for tr in table.find_all('tr'):
            for idx2, td in enumerate(tr.find_all('td')):
                if idx2 != 1:
                    continue
                print(idx2, td)


def realestate_molit(keywords_list):
    cnt = 0
    r = request_and_get('http://www.molit.go.kr/USR/NEWS/m_71/lst.jsp')
    if r is None:
        return
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day-1)
    soup = BeautifulSoup(r.text, 'html.parser')
    for tbody in soup.find_all('tbody'):
        for tr in tbody.find_all('tr'):
            for idx, td in enumerate(tr.find_all('td')):
                if idx == 3:
                    article_date = td.text
                    break
            try:
                tr.a['href']
            except TypeError:
                continue

            if not article_date.startswith(today):
                continue
            if cnt == 0:
                print('\nğŸ“° êµ­í† êµí†µë¶€ ë³´ë„ìë£Œ')
            cnt += 1
            href = 'http://www.molit.go.kr/USR/NEWS/m_71/%s' % tr.a['href']
            print(tr.a.text.strip())
            print(href)


def realestate_thebell(keywords_list):
    base_url = 'https://www.thebell.co.kr/free/content'
    cnt = 0
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day-1)
    for i in count(1):
        driver.implicitly_wait(3)
        url = 'https://www.thebell.co.kr/free/content/article.asp?page=%d&svccode=00' % i
        driver.get(url)
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        for list_box in soup.find_all(match_soup_class(['listBox'])):
            for dl in list_box.find_all('dl'):
                for idx, dd in enumerate(dl.find_all('dd')):
                    if idx == 1:
                        article_date = dd.find('span', attrs={'class': 'date'}).text
                        break
                if article_date is None:
                    continue
                if not article_date.startswith(today):
                    return
                dt = dl.find('dt')
                title = dt.text
                if (title.find('ë¶€ë™ì‚°') == -1 and
                    title.find('ì²­ì•½') == -1 and
                    title.find('ì•„íŒŒíŠ¸') == -1 and
                    title.find('ì¬ê±´ì¶•') == -1 and
                    title.find('ì§‘ê°’') == -1 and
                    title.find('ê±´ì¶•') == -1):
                    # ignore not realestate title
                    continue
                if cnt == 0:
                    print('\nğŸ“° the bell')
                cnt += 1
                href = '%s/%s' % (base_url, dl.a['href'])
                print(title)
                print(href)

def main():
    keywords_list = []
    today = '%4d-%02d-%02d' % (now.year, now.month, now.day)
# í• ê±°
# ì•„ì‹œì•„ ê²½ì œ, http://www.asiae.co.kr/news/realty/
# ê±´ì„¤íƒ€ì„ì¦ˆ, http://www.constimes.co.kr/news/articleList.html?sc_section_code=S1N3

# ì¡°ì§€ì•„, http://gakara.org/news/documents.html
# ë‰´ì§ˆëœë“œ, http://www.inztimes.com/category/property/property_news/
# ë²¤ì¿ ë²„ http://www.vanchosun.com/realestate/main/frame.php?main=newinfo
# í† ë¡ í†  http://www.canadabesthouse.com/pages/newsAll.php?listingType=business
# ë² íŠ¸ë‚¨ http://www.vinahanin.com/batdongsannews

    realestate_thebell(keywords_list)
    # realestate_molit(keywords_list)
    # realestate_venchosun(keywords_list)
    # realestate_cak(keywords_list)
    # realestate_sedaily(keywords_list)
    # realestate_cnews(keywords_list)
    # realestate_mbn(keywords_list)
    # realestate_chosun(keywords_list)
    # realestate_hankyung(keywords_list)

    # realestate_gyunghyang(keywords_list)
    # realestate_kookmin(keywords_list)
    # realestate_nocut(keywords_list)
    # realestate_donga(keywords_list)
    # realestate_moonhwa(keywords_list)
    # realestate_segye(keywords_list)
    # realestate_joins(keywords_list)
    # realestate_hani(keywords_list)

    # realestate_naver(keywords_list)
    # realestate_daum(keywords_list)
    driver.quit()

if __name__ == '__main__':
    main()
